{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11275c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For batch Gd and many other optimizations where we need to process data in batches we use Dataloaders\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "#method1 : manual creation of batches and all manually\n",
    "batch_size = 30\n",
    "num_epochs = 100\n",
    "n_samples = X_train.size()[0]\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    permutation = torch.randperm(X_train.size()[0]) #shuffle the data\n",
    "    for i in range(0, X_train.size()[0], batch_size):\n",
    "        indices = permutation[i:i+batch_size]\n",
    "        batch_x, batch_y = X_train[indices], y_train[indices]\n",
    "\n",
    "        #forward pass\n",
    "        y_pred = model.forward(batch_x)\n",
    "\n",
    "        #compute loss\n",
    "        loss = criterion(y_pred, batch_y)\n",
    "\n",
    "        #backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "#issue is that we have to manually handle the batching,shuffling etc. This can be simplified using Dataloaders\n",
    "\n",
    "\n",
    "#method 2 : using Dataloaders\n",
    "\n",
    "#DATASET: It defines how to access the data. We can create custom datasets by inheriting torch.utils.data.Dataset class\n",
    "#Here we use TensorDataset which is a dataset wrapping tensors. Each sample will be retrieved by indexing tensors along the first dimension.\n",
    "\n",
    "#DATALOADER: It provides an iterable over the given dataset. It can handle batching,shuffling and loading the data in parallel using multiprocessing workers.\n",
    "#here we create a DataLoader for our training data with specified batch size and shuffling enabled. \n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "print(train_dataset.__len__())  #gives number of samples in the dataset\n",
    "print(train_dataset[0])    #gives the first sample (features,label)(in our custom dataset fxn if we create we can apply any preprocessing here inside that class fxn)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        #forward pass\n",
    "        y_pred = model.forward(batch_x)\n",
    "\n",
    "        #compute loss\n",
    "        loss = criterion(y_pred, batch_y)\n",
    "\n",
    "        #backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PYTORCH-LOCAL",
   "language": "python",
   "name": "pytorch-local"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
